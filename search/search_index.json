{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IBM Cloud Kubernetes Service Lab \u00b6 An introduction to containers \u00b6 Hey, are you looking for a containers 101 course? Check out our Docker Essentials . Containers allow you to run securely isolated applications with quotas on system resources. Containers started out as an individual feature delivered with the linux kernel. Docker launched with making containers easy to use and developers quickly latched onto that idea. Containers have also sparked an interest in microservice architecture, a design pattern for developing applications in which complex applications are down into smaller, composable pieces which work together. Watch this video to learn about production uses of containers. Objectives \u00b6 This lab is an introduction to using Docker containers on Kubernetes in the IBM Cloud Kubernetes Service. By the end of the course, you'll achieve these objectives: Understand core concepts of Kubernetes Build a Docker image and deploy an application on Kubernetes in the IBM Cloud Kubernetes Service Control application deployments, while minimizing your time with infrastructure management Add AI services to extend your app Secure and monitor your cluster and app Prerequisites \u00b6 A Pay-As-You-Go or Subscription IBM Cloud account Virtual machines \u00b6 Prior to containers, most infrastructure ran not on bare metal, but atop hypervisors managing multiple virtualized operating systems (OSes). This arrangement allowed isolation of applications from one another on a higher level than that provided by the OS. These virtualized operating systems see what looks like their own exclusive hardware. However, this also means that each of these virtual operating systems are replicating an entire OS, taking up disk space. Containers \u00b6 Containers provide isolation similar to VMs, except provided by the OS and at the process level. Each container is a process or group of processes run in isolation. Typical containers explicitly run only a single process, as they have no need for the standard system services. What they usually need to do can be provided by system calls to the base OS kernel. The isolation on linux is provided by a feature called 'namespaces'. Each different kind of isolation (IE user, cgroups) is provided by a different namespace. This is a list of some of the namespaces that are commonly used and visible to the user: PID - process IDs USER - user and group IDs UTS - hostname and domain name NS - mount points NET - network devices, stacks, and ports CGROUPS - control limits and monitoring of resources VM vs container \u00b6 Traditional applications are run on native hardware. A single application does not typically use the full resources of a single machine. We try to run multiple applications on a single machine to avoid wasting resources. We could run multiple copies of the same application, but to provide isolation we use VMs to run multiple application instances (VMs) on the same hardware. These VMs have full operating system stacks which make them relatively large and inefficient due to duplication both at runtime and on disk. Containers allow you to share the host OS. This reduces duplication while still providing the isolation. Containers also allow you to drop unneeded files such as system libraries and binaries to save space and reduce your attack surface. If SSHD or LIBC are not installed, they cannot be exploited. Get set up \u00b6 Before we dive into Kubernetes, you need to provision a cluster for your containerized app. Then you won't have to wait for it to be ready for the subsequent labs. You must install the CLIs per https://console.ng.bluemix.net/docs/containers/cs_cli_install.html . If you do not yet have these CLIs and the Kubernetes CLI, do lab 0 before starting the course. If you haven't already, provision a cluster. This can take a few minutes, so let it start first: ibmcloud cs cluster-create --name <name-of-cluster> After creation, before using the cluster, make sure it has completed provisioning and is ready for use. Run ibmcloud cs clusters and make sure that your cluster is in state \"deployed\". Then use ibmcloud cs workers <name-of-cluster> and make sure that all worker nodes are in state \"normal\" with Status \"Ready\". Kubernetes and containers: an overview \u00b6 Let's talk about Kubernetes orchestration for containers before we build an application on it. We need to understand the following facts about it: What is Kubernetes, exactly? How was Kubernetes created? Kubernetes architecture Kubernetes resource model Kubernetes at IBM Let's get started What is Kubernetes? \u00b6 Now that we know what containers are, let's define what Kubernetes is. Kubernetes is a container orchestrator to provision, manage, and scale applications. In other words, Kubernetes allows you to manage the lifecycle of containerized applications within a cluster of nodes (which are a collection of worker machines, for example, VMs, physical machines etc.). Your applications may need many other resources to run such as Volumes, Networks, and Secrets that will help you to do things such as connect to databases, talk to firewalled backends, and secure keys. Kubernetes helps you add these resources into your application. Infrastructure resources needed by applications are managed declaratively. Fast fact: Other orchestration technologies are Mesos and Swarm. The key paradigm of kubernetes is it\u2019s Declarative model. The user provides the \"desired state\" and Kubernetes will do it's best make it happen. If you need 5 instances, you do not start 5 separate instances on your own but rather tell Kubernetes that you need 5 instances and Kubernetes will reconcile the state automatically. Simply at this point you need to know that you declare the state you want and Kubernetes makes that happen. If something goes wrong with one of your instances and it crashes, Kubernetes still knows the desired state and creates a new instances on an available node. Fun to know: Kubernetes goes by many names. Sometimes it is shortened to k8s (losing the internal 8 letters), or kube . The word is rooted in ancient Greek and means \"Helmsman\". A helmsman is the person who steers a ship. We hope you can seen the analogy between directing a ship and the decisions made to orchestrate containers on a cluster. How was Kubernetes created? \u00b6 Google wanted to open source their knowledge of creating and running the internal tools Borg & Omega. It adopted Open Governance for Kubernetes by starting the Cloud Native Computing Foundation (CNCF) and giving Kubernetes to that foundation, therefore making it less influenced by Google directly. Many companies such as RedHat, Microsoft, IBM and Amazon quickly joined the foundation. Main entry point for the kubernetes project is at http://kubernetes.io and the source code can be found at https://github.com/kubernetes . Kubernetes architecture \u00b6 At its core, Kubernetes is a data store (etcd). The declarative model is stored in the data store as objects, that means when you say I want 5 instances of a container then that request is stored into the data store. This information change is watched and delegated to Controllers to take action. Controllers then react to the model and attempt to take action to achieve the desired state. The power of Kubernetes is in its simplistic model. As shown, API server is a simple HTTP server handling create/read/update/delete(CRUD) operations on the data store. Then the controller picks up the change you wanted and makes that happen. Controllers are responsible for instantiating the actual resource represented by any Kubernetes resource. These actual resources are what your application needs to allow it to run successfully. Kubernetes resource model \u00b6 Kubernetes Infrastructure defines a resource for every purpose. Each resource is monitored and processed by a controller. When you define your application, it contains a collection of these resources. This collection will then be read by Controllers to build your applications actual backing instances. Some of resources that you may work with are listed below for your reference, for a full list you should go to https://kubernetes.io/docs/concepts/ . In this class we will only use a few of them, like Pod, Deployment, etc. Config Maps holds configuration data for pods to consume. Daemon Sets ensure that each node in the cluster runs this Pod Deployments defines a desired state of a deployment object Events provides lifecycle events on Pods and other deployment objects Endpoints allows a inbound connections to reach the cluster services Ingress is a collection of rules that allow inbound connections to reach the cluster services Jobs creates one or more pods and as they complete successfully the job is marked as completed. Node is a worker machine in Kubernetes Namespaces are multiple virtual clusters backed by the same physical cluster Pods are the smallest deployable units of computing that can be created and managed in Kubernetes Persistent Volumes provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed Replica Sets ensures that a specified number of pod replicas are running at any given time Secrets are intended to hold sensitive information, such as passwords, OAuth tokens, and ssh keys Service Accounts provides an identity for processes that run in a Pod Services is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. Stateful Sets is the workload API object used to manage stateful applications. and more... Kubernetes does not have the concept of an application. It has simple building blocks that you are required to compose. Kubernetes is a cloud native platform where the internal resource model is the same as the end user resource model. Key resources \u00b6 A Pod is the smallest object model that you can create and run. You can add labels to a pod to identify a subset to run operations on. When you are ready to scale your application you can use the label to tell Kubernetes which Pod you need to scale. A Pod typically represent a process in your cluster. Pods contain at least one container that runs the job and additionally may have other containers in it called sidecars for monitoring, logging, etc. Essentially a Pod is a group of containers. When we talk about a application, we usually refer to group of Pods. Although an entire application can be run in a single Pod, we usually build multiple Pods that talk to each other to make a useful application. We will see why separating the application logic and backend database into separate Pods will scale better when we build an application shortly. Services define how to expose your app as a DNS entry to have a stable reference. We use query based selector to choose which pods are supplying that service. The user directly manipulates resources via yaml: kubectl ( create | get | apply | delete ) -f myResource.yaml Kubernetes provides us with a client interface through \u2018kubectl\u2019. Kubectl commands allow you to manage your applications, manage cluster and cluster resources, by modifying the model in the data store. Kubernetes application deployment workflow \u00b6 User via \"kubectl\" deploys a new application. Kubectl sends the request to the API Server. API server receives the request and stores it in the data store (etcd). Once the request is written to data store, the API server is done with the request. Watchers detects the resource changes and send a notification to controller to act upon it Controller detects the new app and creates new pods to match the desired number# of instances. Any changes to the stored model will be picked up to create or delete Pods. Scheduler assigns new pods to a Node based on a criteria. Scheduler makes decisions to run Pods on specific Nodes in the cluster. Scheduler modifies the model with the node information. Kubelet on a node detects a pod with an assignment to itself, and deploys the requested containers via the container runtime (e.g. Docker). Each Node watches the storage to see what pods it is assigned to run. It takes necessary actions on resource assigned to it like create/delete Pods. Kubeproxy manages network traffic for the pods - including service discovery and load-balancing. Kubeproxy is responsible for communication between Pods that want to interact. Lab information \u00b6 IBM Cloud provides the capability to run applications in containers on Kubernetes. The IBM Cloud Kubernetes Service runs Kubernetes clusters which deliver the following: Powerful tools Intuitive user experience Built-in security and isolation to enable rapid delivery of secure applications Cloud services including cognitive capabilities from Watson Capability to manage dedicated cluster resources for both stateless applications and stateful workloads Lab overview \u00b6 Lab 0 (Optional): Provides a walkthrough for installing IBM Cloud command-line tools and the Kubernetes CLI. You can skip this lab if you have the IBM Cloud CLI, the container-service plugin, the containers-registry plugin, and the kubectl CLI already installed on your machine. Lab 1 : This lab walks through creating and deploying a simple \"guestbook\" app written in Go as a net/http Server and accessing it. Lab 2 : Builds on lab 1 to expand to a more resilient setup which can survive having containers fail and recover. Lab 2 will also walk through basic services you need to get started with Kubernetes and the IBM Cloud Kubernetes Service Lab 3 : Builds on lab 2 by increasing the capabilities of the deployed Guestbook application. This lab covers basic distributed application design and how kubernetes helps you use standard design practices. Lab 4 : How to enable your application so Kubernetes can automatically monitor and recover your applications with no user intervention. Lab D : Debugging tips and tricks to help you along your Kubernetes journey. This lab is useful reference that does not follow in a specific sequence of the other labs.","title":"About the workshop"},{"location":"#ibm-cloud-kubernetes-service-lab","text":"","title":"IBM Cloud Kubernetes Service Lab"},{"location":"#an-introduction-to-containers","text":"Hey, are you looking for a containers 101 course? Check out our Docker Essentials . Containers allow you to run securely isolated applications with quotas on system resources. Containers started out as an individual feature delivered with the linux kernel. Docker launched with making containers easy to use and developers quickly latched onto that idea. Containers have also sparked an interest in microservice architecture, a design pattern for developing applications in which complex applications are down into smaller, composable pieces which work together. Watch this video to learn about production uses of containers.","title":"An introduction to containers"},{"location":"#objectives","text":"This lab is an introduction to using Docker containers on Kubernetes in the IBM Cloud Kubernetes Service. By the end of the course, you'll achieve these objectives: Understand core concepts of Kubernetes Build a Docker image and deploy an application on Kubernetes in the IBM Cloud Kubernetes Service Control application deployments, while minimizing your time with infrastructure management Add AI services to extend your app Secure and monitor your cluster and app","title":"Objectives"},{"location":"#prerequisites","text":"A Pay-As-You-Go or Subscription IBM Cloud account","title":"Prerequisites"},{"location":"#virtual-machines","text":"Prior to containers, most infrastructure ran not on bare metal, but atop hypervisors managing multiple virtualized operating systems (OSes). This arrangement allowed isolation of applications from one another on a higher level than that provided by the OS. These virtualized operating systems see what looks like their own exclusive hardware. However, this also means that each of these virtual operating systems are replicating an entire OS, taking up disk space.","title":"Virtual machines"},{"location":"#containers","text":"Containers provide isolation similar to VMs, except provided by the OS and at the process level. Each container is a process or group of processes run in isolation. Typical containers explicitly run only a single process, as they have no need for the standard system services. What they usually need to do can be provided by system calls to the base OS kernel. The isolation on linux is provided by a feature called 'namespaces'. Each different kind of isolation (IE user, cgroups) is provided by a different namespace. This is a list of some of the namespaces that are commonly used and visible to the user: PID - process IDs USER - user and group IDs UTS - hostname and domain name NS - mount points NET - network devices, stacks, and ports CGROUPS - control limits and monitoring of resources","title":"Containers"},{"location":"#vm-vs-container","text":"Traditional applications are run on native hardware. A single application does not typically use the full resources of a single machine. We try to run multiple applications on a single machine to avoid wasting resources. We could run multiple copies of the same application, but to provide isolation we use VMs to run multiple application instances (VMs) on the same hardware. These VMs have full operating system stacks which make them relatively large and inefficient due to duplication both at runtime and on disk. Containers allow you to share the host OS. This reduces duplication while still providing the isolation. Containers also allow you to drop unneeded files such as system libraries and binaries to save space and reduce your attack surface. If SSHD or LIBC are not installed, they cannot be exploited.","title":"VM vs container"},{"location":"#get-set-up","text":"Before we dive into Kubernetes, you need to provision a cluster for your containerized app. Then you won't have to wait for it to be ready for the subsequent labs. You must install the CLIs per https://console.ng.bluemix.net/docs/containers/cs_cli_install.html . If you do not yet have these CLIs and the Kubernetes CLI, do lab 0 before starting the course. If you haven't already, provision a cluster. This can take a few minutes, so let it start first: ibmcloud cs cluster-create --name <name-of-cluster> After creation, before using the cluster, make sure it has completed provisioning and is ready for use. Run ibmcloud cs clusters and make sure that your cluster is in state \"deployed\". Then use ibmcloud cs workers <name-of-cluster> and make sure that all worker nodes are in state \"normal\" with Status \"Ready\".","title":"Get set up"},{"location":"#kubernetes-and-containers-an-overview","text":"Let's talk about Kubernetes orchestration for containers before we build an application on it. We need to understand the following facts about it: What is Kubernetes, exactly? How was Kubernetes created? Kubernetes architecture Kubernetes resource model Kubernetes at IBM Let's get started","title":"Kubernetes and containers: an overview"},{"location":"#what-is-kubernetes","text":"Now that we know what containers are, let's define what Kubernetes is. Kubernetes is a container orchestrator to provision, manage, and scale applications. In other words, Kubernetes allows you to manage the lifecycle of containerized applications within a cluster of nodes (which are a collection of worker machines, for example, VMs, physical machines etc.). Your applications may need many other resources to run such as Volumes, Networks, and Secrets that will help you to do things such as connect to databases, talk to firewalled backends, and secure keys. Kubernetes helps you add these resources into your application. Infrastructure resources needed by applications are managed declaratively. Fast fact: Other orchestration technologies are Mesos and Swarm. The key paradigm of kubernetes is it\u2019s Declarative model. The user provides the \"desired state\" and Kubernetes will do it's best make it happen. If you need 5 instances, you do not start 5 separate instances on your own but rather tell Kubernetes that you need 5 instances and Kubernetes will reconcile the state automatically. Simply at this point you need to know that you declare the state you want and Kubernetes makes that happen. If something goes wrong with one of your instances and it crashes, Kubernetes still knows the desired state and creates a new instances on an available node. Fun to know: Kubernetes goes by many names. Sometimes it is shortened to k8s (losing the internal 8 letters), or kube . The word is rooted in ancient Greek and means \"Helmsman\". A helmsman is the person who steers a ship. We hope you can seen the analogy between directing a ship and the decisions made to orchestrate containers on a cluster.","title":"What is Kubernetes?"},{"location":"#how-was-kubernetes-created","text":"Google wanted to open source their knowledge of creating and running the internal tools Borg & Omega. It adopted Open Governance for Kubernetes by starting the Cloud Native Computing Foundation (CNCF) and giving Kubernetes to that foundation, therefore making it less influenced by Google directly. Many companies such as RedHat, Microsoft, IBM and Amazon quickly joined the foundation. Main entry point for the kubernetes project is at http://kubernetes.io and the source code can be found at https://github.com/kubernetes .","title":"How was Kubernetes created?"},{"location":"#kubernetes-architecture","text":"At its core, Kubernetes is a data store (etcd). The declarative model is stored in the data store as objects, that means when you say I want 5 instances of a container then that request is stored into the data store. This information change is watched and delegated to Controllers to take action. Controllers then react to the model and attempt to take action to achieve the desired state. The power of Kubernetes is in its simplistic model. As shown, API server is a simple HTTP server handling create/read/update/delete(CRUD) operations on the data store. Then the controller picks up the change you wanted and makes that happen. Controllers are responsible for instantiating the actual resource represented by any Kubernetes resource. These actual resources are what your application needs to allow it to run successfully.","title":"Kubernetes architecture"},{"location":"#kubernetes-resource-model","text":"Kubernetes Infrastructure defines a resource for every purpose. Each resource is monitored and processed by a controller. When you define your application, it contains a collection of these resources. This collection will then be read by Controllers to build your applications actual backing instances. Some of resources that you may work with are listed below for your reference, for a full list you should go to https://kubernetes.io/docs/concepts/ . In this class we will only use a few of them, like Pod, Deployment, etc. Config Maps holds configuration data for pods to consume. Daemon Sets ensure that each node in the cluster runs this Pod Deployments defines a desired state of a deployment object Events provides lifecycle events on Pods and other deployment objects Endpoints allows a inbound connections to reach the cluster services Ingress is a collection of rules that allow inbound connections to reach the cluster services Jobs creates one or more pods and as they complete successfully the job is marked as completed. Node is a worker machine in Kubernetes Namespaces are multiple virtual clusters backed by the same physical cluster Pods are the smallest deployable units of computing that can be created and managed in Kubernetes Persistent Volumes provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed Replica Sets ensures that a specified number of pod replicas are running at any given time Secrets are intended to hold sensitive information, such as passwords, OAuth tokens, and ssh keys Service Accounts provides an identity for processes that run in a Pod Services is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. Stateful Sets is the workload API object used to manage stateful applications. and more... Kubernetes does not have the concept of an application. It has simple building blocks that you are required to compose. Kubernetes is a cloud native platform where the internal resource model is the same as the end user resource model.","title":"Kubernetes resource model"},{"location":"#key-resources","text":"A Pod is the smallest object model that you can create and run. You can add labels to a pod to identify a subset to run operations on. When you are ready to scale your application you can use the label to tell Kubernetes which Pod you need to scale. A Pod typically represent a process in your cluster. Pods contain at least one container that runs the job and additionally may have other containers in it called sidecars for monitoring, logging, etc. Essentially a Pod is a group of containers. When we talk about a application, we usually refer to group of Pods. Although an entire application can be run in a single Pod, we usually build multiple Pods that talk to each other to make a useful application. We will see why separating the application logic and backend database into separate Pods will scale better when we build an application shortly. Services define how to expose your app as a DNS entry to have a stable reference. We use query based selector to choose which pods are supplying that service. The user directly manipulates resources via yaml: kubectl ( create | get | apply | delete ) -f myResource.yaml Kubernetes provides us with a client interface through \u2018kubectl\u2019. Kubectl commands allow you to manage your applications, manage cluster and cluster resources, by modifying the model in the data store.","title":"Key resources"},{"location":"#kubernetes-application-deployment-workflow","text":"User via \"kubectl\" deploys a new application. Kubectl sends the request to the API Server. API server receives the request and stores it in the data store (etcd). Once the request is written to data store, the API server is done with the request. Watchers detects the resource changes and send a notification to controller to act upon it Controller detects the new app and creates new pods to match the desired number# of instances. Any changes to the stored model will be picked up to create or delete Pods. Scheduler assigns new pods to a Node based on a criteria. Scheduler makes decisions to run Pods on specific Nodes in the cluster. Scheduler modifies the model with the node information. Kubelet on a node detects a pod with an assignment to itself, and deploys the requested containers via the container runtime (e.g. Docker). Each Node watches the storage to see what pods it is assigned to run. It takes necessary actions on resource assigned to it like create/delete Pods. Kubeproxy manages network traffic for the pods - including service discovery and load-balancing. Kubeproxy is responsible for communication between Pods that want to interact.","title":"Kubernetes application deployment workflow"},{"location":"#lab-information","text":"IBM Cloud provides the capability to run applications in containers on Kubernetes. The IBM Cloud Kubernetes Service runs Kubernetes clusters which deliver the following: Powerful tools Intuitive user experience Built-in security and isolation to enable rapid delivery of secure applications Cloud services including cognitive capabilities from Watson Capability to manage dedicated cluster resources for both stateless applications and stateful workloads","title":"Lab information"},{"location":"#lab-overview","text":"Lab 0 (Optional): Provides a walkthrough for installing IBM Cloud command-line tools and the Kubernetes CLI. You can skip this lab if you have the IBM Cloud CLI, the container-service plugin, the containers-registry plugin, and the kubectl CLI already installed on your machine. Lab 1 : This lab walks through creating and deploying a simple \"guestbook\" app written in Go as a net/http Server and accessing it. Lab 2 : Builds on lab 1 to expand to a more resilient setup which can survive having containers fail and recover. Lab 2 will also walk through basic services you need to get started with Kubernetes and the IBM Cloud Kubernetes Service Lab 3 : Builds on lab 2 by increasing the capabilities of the deployed Guestbook application. This lab covers basic distributed application design and how kubernetes helps you use standard design practices. Lab 4 : How to enable your application so Kubernetes can automatically monitor and recover your applications with no user intervention. Lab D : Debugging tips and tricks to help you along your Kubernetes journey. This lab is useful reference that does not follow in a specific sequence of the other labs.","title":"Lab overview"},{"location":"CONTRIBUTING/","text":"Contributing In General \u00b6 Our project welcomes external contributions! If you have an itch, please feel free to scratch it. To contribute code or documentation, please submit a pull request to the GitHub repository . A good way to familiarize yourself with the codebase and contribution process is to look for and tackle low-hanging fruit in the issue tracker . Before embarking on a more ambitious contribution, please quickly get in touch with us via an issue. We appreciate your effort, and want to avoid a situation where a contribution requires extensive rework (by you or by us), sits in the queue for a long time, or cannot be accepted at all! Proposing new features \u00b6 If you would like to implement a new feature, please raise an issue before sending a pull request so the feature can be discussed. This is to avoid you spending your valuable time working on a feature that the project developers are not willing to accept into the code base. Fixing bugs \u00b6 If you would like to fix a bug, please raise an issue before sending a pull request so it can be discussed. If the fix is trivial or non controversial then this is not usually necessary. Merge approval \u00b6 The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from two of the maintainers of each component affected. Note that if your initial push does not pass TravisCI your change will not be approved. For more details, see the MAINTAINERS page.","title":"Contributing In General"},{"location":"CONTRIBUTING/#contributing-in-general","text":"Our project welcomes external contributions! If you have an itch, please feel free to scratch it. To contribute code or documentation, please submit a pull request to the GitHub repository . A good way to familiarize yourself with the codebase and contribution process is to look for and tackle low-hanging fruit in the issue tracker . Before embarking on a more ambitious contribution, please quickly get in touch with us via an issue. We appreciate your effort, and want to avoid a situation where a contribution requires extensive rework (by you or by us), sits in the queue for a long time, or cannot be accepted at all!","title":"Contributing In General"},{"location":"CONTRIBUTING/#proposing-new-features","text":"If you would like to implement a new feature, please raise an issue before sending a pull request so the feature can be discussed. This is to avoid you spending your valuable time working on a feature that the project developers are not willing to accept into the code base.","title":"Proposing new features"},{"location":"CONTRIBUTING/#fixing-bugs","text":"If you would like to fix a bug, please raise an issue before sending a pull request so it can be discussed. If the fix is trivial or non controversial then this is not usually necessary.","title":"Fixing bugs"},{"location":"CONTRIBUTING/#merge-approval","text":"The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from two of the maintainers of each component affected. Note that if your initial push does not pass TravisCI your change will not be approved. For more details, see the MAINTAINERS page.","title":"Merge approval"},{"location":"MAINTAINERS/","text":"Maintainers Guide \u00b6 This guide is intended for maintainers - anybody with commit access to one or more Developer Technology repositories. Maintainers \u00b6 Name GitHub email Nathan Fritze nfritze nfritz@us.ibm.com Nathan LeViere nathanleviere njlevier@gmail.com Methodoology \u00b6 A master branch. This branch MUST be releasable at all times. Commits and merges against this branch MUST contain only bugfixes and/or security fixes. Maintenance releases are tagged against master. A develop branch. This branch contains your proposed changes The remainder of this document details how to merge pull requests to the repositories. Merge approval \u00b6 The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from one of the maintainers of each component affected. Reviewing Pull Requests \u00b6 We recommend reviewing pull requests directly within GitHub. This allows a public commentary on changes, providing transparency for all users. When providing feedback be civil, courteous, and kind. Disagreement is fine, so long as the discourse is carried out politely. If we see a record of uncivil or abusive comments, we will revoke your commit privileges and invite you to leave the project. During your review, consider the following points: Does the change have impact? \u00b6 While fixing typos is nice as it adds to the overall quality of the project, merging a typo fix at a time can be a waste of effort. (Merging many typo fixes because somebody reviewed the entire component, however, is useful!) Other examples to be wary of: Changes in variable names. Ask whether or not the change will make understanding the code easier, or if it could simply a personal preference on the part of the author. Essentially: feel free to close issues that do not have impact. Do the changes make sense? \u00b6 If you do not understand what the changes are or what they accomplish, ask the author for clarification. Ask the author to add comments and/or clarify test case names to make the intentions clear. At times, such clarification will reveal that the author may not be using the code correctly, or is unaware of features that accommodate their needs. If you feel this is the case, work up a code sample that would address the issue for them, and feel free to close the issue once they confirm. Is this a new feature? If so \u00b6 Does the issue contain narrative indicating the need for the feature? If not, ask them to provide that information. Since the issue will be linked in the changelog, this will often be a user's first introduction to it. Are new unit tests in place that test all new behaviors introduced? If not, do not merge the feature until they are! Is documentation in place for the new feature? (See the documentation guidelines). If not do not merge the feature until it is! Is the feature necessary for general use cases? Try and keep the scope of any given component narrow. If a proposed feature does not fit that scope, recommend to the user that they maintain the feature on their own, and close the request. You may also recommend that they see if the feature gains traction amongst other users, and suggest they re-submit when they can show such support.","title":"Maintainers Guide"},{"location":"MAINTAINERS/#maintainers-guide","text":"This guide is intended for maintainers - anybody with commit access to one or more Developer Technology repositories.","title":"Maintainers Guide"},{"location":"MAINTAINERS/#maintainers","text":"Name GitHub email Nathan Fritze nfritze nfritz@us.ibm.com Nathan LeViere nathanleviere njlevier@gmail.com","title":"Maintainers"},{"location":"MAINTAINERS/#methodoology","text":"A master branch. This branch MUST be releasable at all times. Commits and merges against this branch MUST contain only bugfixes and/or security fixes. Maintenance releases are tagged against master. A develop branch. This branch contains your proposed changes The remainder of this document details how to merge pull requests to the repositories.","title":"Methodoology"},{"location":"MAINTAINERS/#merge-approval","text":"The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from one of the maintainers of each component affected.","title":"Merge approval"},{"location":"MAINTAINERS/#reviewing-pull-requests","text":"We recommend reviewing pull requests directly within GitHub. This allows a public commentary on changes, providing transparency for all users. When providing feedback be civil, courteous, and kind. Disagreement is fine, so long as the discourse is carried out politely. If we see a record of uncivil or abusive comments, we will revoke your commit privileges and invite you to leave the project. During your review, consider the following points:","title":"Reviewing Pull Requests"},{"location":"MAINTAINERS/#does-the-change-have-impact","text":"While fixing typos is nice as it adds to the overall quality of the project, merging a typo fix at a time can be a waste of effort. (Merging many typo fixes because somebody reviewed the entire component, however, is useful!) Other examples to be wary of: Changes in variable names. Ask whether or not the change will make understanding the code easier, or if it could simply a personal preference on the part of the author. Essentially: feel free to close issues that do not have impact.","title":"Does the change have impact?"},{"location":"MAINTAINERS/#do-the-changes-make-sense","text":"If you do not understand what the changes are or what they accomplish, ask the author for clarification. Ask the author to add comments and/or clarify test case names to make the intentions clear. At times, such clarification will reveal that the author may not be using the code correctly, or is unaware of features that accommodate their needs. If you feel this is the case, work up a code sample that would address the issue for them, and feel free to close the issue once they confirm.","title":"Do the changes make sense?"},{"location":"MAINTAINERS/#is-this-a-new-feature-if-so","text":"Does the issue contain narrative indicating the need for the feature? If not, ask them to provide that information. Since the issue will be linked in the changelog, this will often be a user's first introduction to it. Are new unit tests in place that test all new behaviors introduced? If not, do not merge the feature until they are! Is documentation in place for the new feature? (See the documentation guidelines). If not do not merge the feature until it is! Is the feature necessary for general use cases? Try and keep the scope of any given component narrow. If a proposed feature does not fit that scope, recommend to the user that they maintain the feature on their own, and close the request. You may also recommend that they see if the feature gains traction amongst other users, and suggest they re-submit when they can show such support.","title":"Is this a new feature? If so"},{"location":"SUMMARY/","text":"Summary \u00b6 Getting Started \u00b6 Lab 0: Get the IBM Cloud Container Service Labs \u00b6 Lab 1. Set up and deploy your first application Lab 2: Scale and Update Deployments Lab 3: Scale and update apps natively, building multi-tier applications Resources \u00b6 IBM Developer","title":"Summary"},{"location":"SUMMARY/#summary","text":"","title":"Summary"},{"location":"SUMMARY/#getting-started","text":"Lab 0: Get the IBM Cloud Container Service","title":"Getting Started"},{"location":"SUMMARY/#labs","text":"Lab 1. Set up and deploy your first application Lab 2: Scale and Update Deployments Lab 3: Scale and update apps natively, building multi-tier applications","title":"Labs"},{"location":"SUMMARY/#resources","text":"IBM Developer","title":"Resources"},{"location":"Lab0/","text":"Lab 0. Access a Kubernetes cluster \u00b6 Set up your kubernetes environment \u00b6 For the hands-on labs in this tutorial repository, you will need a kubernetes cluster. One option for creating a cluster is to make use of the Kubernetes as-a-service from the IBM Cloud Kubernetes Service as outlined below. Use the IBM Cloud Kubernetes Service \u00b6 You will need either a paid IBM Cloud account or an IBM Cloud account which is a Trial account (not a Lite account). If you have one of these accounts, use the Getting Started Guide to create your cluster. Use a hosted trial environment \u00b6 There are a few services that are accessible over the Internet for temporary use. As these are free services, they can sometimes experience periods of limited availablity/quality. On the other hand, they can be a quick way to get started! Play with Kubernetes After signing in with your github or docker hub id, click on Start , then Add New Instance and follow steps shown in terminal to spin up the cluster and add workers. Set up on your own workstation \u00b6 If you would like to configure kubernetes to run on your local workstation for non-production, learning use, there are several options. Minikube This solution requires the installation of a supported VM provider (KVM, VirtualBox, HyperKit, Hyper-V - depending on platform) Kubernetes in Docker (kind) Runs a kubernetes cluster on Docker containers Docker Desktop (Mac) Docker Desktop (Windows) Docker Desktop includes a kubernetes environment Microk8s Installable kubernetes packaged as an Ubuntu snap image. Install the IBM Cloud command-line interface \u00b6 As a prerequisite for the IBM Cloud Kubernetes Service plug-in, install the IBM Cloud command-line interface . Once installed, you can access IBM Cloud from your command-line with the prefix bx . Log in to the IBM Cloud CLI: ibmcloud login . Enter your IBM Cloud credentials when prompted. Note: If you have a federated ID, use ibmcloud login --sso to log in to the IBM Cloud CLI. Enter your user name, and use the provided URL in your CLI output to retrieve your one-time passcode. You know you have a federated ID when the login fails without the --sso and succeeds with the --sso option. Install the IBM Cloud Kubernetes Service plug-in \u00b6 To create Kubernetes clusters and manage worker nodes, install the IBM Cloud Kubernetes Service plug-in: ibmcloud plugin install container-service -r Bluemix Note: The prefix for running commands by using the IBM Cloud Kubernetes Service plug-in is bx cs . To verify that the plug-in is installed properly, run the following command: ibmcloud plugin list The IBM Cloud Kubernetes Service plug-in is displayed in the results as container-service . Download the Kubernetes CLI \u00b6 To view a local version of the Kubernetes dashboard and to deploy apps into your clusters, you will need to install the Kubernetes CLI that corresponds with your operating system: OS X Linux Windows For Windows users: Install the Kubernetes CLI in the same directory as the IBM Cloud CLI. This setup saves you some filepath changes when you run commands later. For OS X and Linux users: Move the executable file to the /usr/local/bin directory using the command mv /<path_to_file>/kubectl /usr/local/bin/kubectl . Make sure that /usr/local/bin is listed in your PATH system variable. $ echo $PATH /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin Convert the binary file to an executable: chmod +x /usr/local/bin/kubectl Configure Kubectl to point to IBM Cloud Kubernetes Service \u00b6 List the clusters in your account: ibmcloud ks clusters Set an environment variable that will be used in subsequent commands in this lab. export CLUSTER_NAME = <your_cluster_name> Configure kubectl to point to your cluster ibmcloud ks cluster config --cluster $CLUSTER_NAME Validate proper configuration kubectl get namespace You should see output similar to the following, if so, then your're ready to continue. NAME STATUS AGE default Active 125m ibm-cert-store Active 121m ibm-system Active 124m kube-node-lease Active 125m kube-public Active 125m kube-system Active 125m Download the Workshop Source Code \u00b6 Repo guestbook has the application that we'll be deploying. While we're not going to build it we will use the deployment configuration files from that repo. Guestbook application has two versions v1 and v2 which we will use to demonstrate some rollout functionality later. All the configuration files we use are under the directory guestbook/v1. Repo kube101 contains the step by step instructions to run the workshop. git clone https://github.com/IBM/guestbook.git git clone https://github.com/IBM/kube101.git","title":"Lab 0. Access a Kubernetes cluster"},{"location":"Lab0/#lab-0-access-a-kubernetes-cluster","text":"","title":"Lab 0. Access a Kubernetes cluster"},{"location":"Lab0/#set-up-your-kubernetes-environment","text":"For the hands-on labs in this tutorial repository, you will need a kubernetes cluster. One option for creating a cluster is to make use of the Kubernetes as-a-service from the IBM Cloud Kubernetes Service as outlined below.","title":"Set up your kubernetes environment"},{"location":"Lab0/#use-the-ibm-cloud-kubernetes-service","text":"You will need either a paid IBM Cloud account or an IBM Cloud account which is a Trial account (not a Lite account). If you have one of these accounts, use the Getting Started Guide to create your cluster.","title":"Use the IBM Cloud Kubernetes Service"},{"location":"Lab0/#use-a-hosted-trial-environment","text":"There are a few services that are accessible over the Internet for temporary use. As these are free services, they can sometimes experience periods of limited availablity/quality. On the other hand, they can be a quick way to get started! Play with Kubernetes After signing in with your github or docker hub id, click on Start , then Add New Instance and follow steps shown in terminal to spin up the cluster and add workers.","title":"Use a hosted trial environment"},{"location":"Lab0/#set-up-on-your-own-workstation","text":"If you would like to configure kubernetes to run on your local workstation for non-production, learning use, there are several options. Minikube This solution requires the installation of a supported VM provider (KVM, VirtualBox, HyperKit, Hyper-V - depending on platform) Kubernetes in Docker (kind) Runs a kubernetes cluster on Docker containers Docker Desktop (Mac) Docker Desktop (Windows) Docker Desktop includes a kubernetes environment Microk8s Installable kubernetes packaged as an Ubuntu snap image.","title":"Set up on your own workstation"},{"location":"Lab0/#install-the-ibm-cloud-command-line-interface","text":"As a prerequisite for the IBM Cloud Kubernetes Service plug-in, install the IBM Cloud command-line interface . Once installed, you can access IBM Cloud from your command-line with the prefix bx . Log in to the IBM Cloud CLI: ibmcloud login . Enter your IBM Cloud credentials when prompted. Note: If you have a federated ID, use ibmcloud login --sso to log in to the IBM Cloud CLI. Enter your user name, and use the provided URL in your CLI output to retrieve your one-time passcode. You know you have a federated ID when the login fails without the --sso and succeeds with the --sso option.","title":"Install the IBM Cloud command-line interface"},{"location":"Lab0/#install-the-ibm-cloud-kubernetes-service-plug-in","text":"To create Kubernetes clusters and manage worker nodes, install the IBM Cloud Kubernetes Service plug-in: ibmcloud plugin install container-service -r Bluemix Note: The prefix for running commands by using the IBM Cloud Kubernetes Service plug-in is bx cs . To verify that the plug-in is installed properly, run the following command: ibmcloud plugin list The IBM Cloud Kubernetes Service plug-in is displayed in the results as container-service .","title":"Install the IBM Cloud Kubernetes Service plug-in"},{"location":"Lab0/#download-the-kubernetes-cli","text":"To view a local version of the Kubernetes dashboard and to deploy apps into your clusters, you will need to install the Kubernetes CLI that corresponds with your operating system: OS X Linux Windows For Windows users: Install the Kubernetes CLI in the same directory as the IBM Cloud CLI. This setup saves you some filepath changes when you run commands later. For OS X and Linux users: Move the executable file to the /usr/local/bin directory using the command mv /<path_to_file>/kubectl /usr/local/bin/kubectl . Make sure that /usr/local/bin is listed in your PATH system variable. $ echo $PATH /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin Convert the binary file to an executable: chmod +x /usr/local/bin/kubectl","title":"Download the Kubernetes CLI"},{"location":"Lab0/#configure-kubectl-to-point-to-ibm-cloud-kubernetes-service","text":"List the clusters in your account: ibmcloud ks clusters Set an environment variable that will be used in subsequent commands in this lab. export CLUSTER_NAME = <your_cluster_name> Configure kubectl to point to your cluster ibmcloud ks cluster config --cluster $CLUSTER_NAME Validate proper configuration kubectl get namespace You should see output similar to the following, if so, then your're ready to continue. NAME STATUS AGE default Active 125m ibm-cert-store Active 121m ibm-system Active 124m kube-node-lease Active 125m kube-public Active 125m kube-system Active 125m","title":"Configure Kubectl to point to IBM Cloud Kubernetes Service"},{"location":"Lab0/#download-the-workshop-source-code","text":"Repo guestbook has the application that we'll be deploying. While we're not going to build it we will use the deployment configuration files from that repo. Guestbook application has two versions v1 and v2 which we will use to demonstrate some rollout functionality later. All the configuration files we use are under the directory guestbook/v1. Repo kube101 contains the step by step instructions to run the workshop. git clone https://github.com/IBM/guestbook.git git clone https://github.com/IBM/kube101.git","title":"Download the Workshop Source Code"},{"location":"Lab1/","text":"Lab 1. Deploy your first application \u00b6 Learn how to deploy an application to a Kubernetes cluster hosted within the IBM Container Service. 0. Prerequisites \u00b6 Make sure you satisfy the prerequisites as outlined in Lab 0 1. Deploy the guestbook application \u00b6 In this part of the lab we will deploy an application called guestbook that has already been built and uploaded to DockerHub under the name ibmcom/guestbook:v1 . Start by running guestbook : kubectl create deployment guestbook --image = ibmcom/guestbook:v1 This action will take a bit of time. To check the status of the running application, you can use $ kubectl get pods . You should see output similar to the following: kubectl get pods Eventually, the status should show up as Running . $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-59bd679fdc-bxdg7 1 /1 Running 0 1m The end result of the run command is not just the pod containing our application containers, but a Deployment resource that manages the lifecycle of those pods. Once the status reads Running , we need to expose that deployment as a service so we can access it through the IP of the worker nodes. The guestbook application listens on port 3000. Run: kubectl expose deployment guestbook --type = \"NodePort\" --port = 3000 To find the port used on that worker node, examine your new service: $ kubectl get service guestbook NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook NodePort 10 .10.10.253 <none> 3000 :31208/TCP 1m We can see that our <nodeport> is 31208 . We can see in the output the port mapping from 3000 inside the pod exposed to the cluster on port 31208. This port in the 31000 range is automatically chosen, and could be different for you. guestbook is now running on your cluster, and exposed to the internet. We need to find out where it is accessible. The worker nodes running in the container service get external IP addresses. Get the workers for your cluster and note one (any one) of the public IPs listed on the <public-IP> line. Replace $CLUSTER_NAME with your cluster name unless you have this environment variable set. $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10 .185.199.3 Ready master,worker 63d v1.16.2+283af84 10 .185.199.3 169 .59.228.215 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 10 .185.199.6 Ready master,worker 63d v1.16.2+283af84 10 .185.199.6 169 .47.78.51 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 We can see that our <EXTERNAL-IP> is 169.59.228.215 . Now that you have both the address and the port, you can now access the application in the web browser at <public-IP>:<nodeport> . In the example case this is 173.193.99.136:31208 . Congratulations, you've now deployed an application to Kubernetes! When you're all done, continue to the next lab of this course .","title":"Lab 1. Deploy your first application"},{"location":"Lab1/#lab-1-deploy-your-first-application","text":"Learn how to deploy an application to a Kubernetes cluster hosted within the IBM Container Service.","title":"Lab 1. Deploy your first application"},{"location":"Lab1/#0-prerequisites","text":"Make sure you satisfy the prerequisites as outlined in Lab 0","title":"0. Prerequisites"},{"location":"Lab1/#1-deploy-the-guestbook-application","text":"In this part of the lab we will deploy an application called guestbook that has already been built and uploaded to DockerHub under the name ibmcom/guestbook:v1 . Start by running guestbook : kubectl create deployment guestbook --image = ibmcom/guestbook:v1 This action will take a bit of time. To check the status of the running application, you can use $ kubectl get pods . You should see output similar to the following: kubectl get pods Eventually, the status should show up as Running . $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-59bd679fdc-bxdg7 1 /1 Running 0 1m The end result of the run command is not just the pod containing our application containers, but a Deployment resource that manages the lifecycle of those pods. Once the status reads Running , we need to expose that deployment as a service so we can access it through the IP of the worker nodes. The guestbook application listens on port 3000. Run: kubectl expose deployment guestbook --type = \"NodePort\" --port = 3000 To find the port used on that worker node, examine your new service: $ kubectl get service guestbook NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook NodePort 10 .10.10.253 <none> 3000 :31208/TCP 1m We can see that our <nodeport> is 31208 . We can see in the output the port mapping from 3000 inside the pod exposed to the cluster on port 31208. This port in the 31000 range is automatically chosen, and could be different for you. guestbook is now running on your cluster, and exposed to the internet. We need to find out where it is accessible. The worker nodes running in the container service get external IP addresses. Get the workers for your cluster and note one (any one) of the public IPs listed on the <public-IP> line. Replace $CLUSTER_NAME with your cluster name unless you have this environment variable set. $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10 .185.199.3 Ready master,worker 63d v1.16.2+283af84 10 .185.199.3 169 .59.228.215 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 10 .185.199.6 Ready master,worker 63d v1.16.2+283af84 10 .185.199.6 169 .47.78.51 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 We can see that our <EXTERNAL-IP> is 169.59.228.215 . Now that you have both the address and the port, you can now access the application in the web browser at <public-IP>:<nodeport> . In the example case this is 173.193.99.136:31208 . Congratulations, you've now deployed an application to Kubernetes! When you're all done, continue to the next lab of this course .","title":"1. Deploy the guestbook application"},{"location":"Lab1/script/script/","text":"Pod \u00b6 In Kubernetes, a group of one or more containers is called a pod. Containers in a pod are deployed together, and are started, stopped, and replicated as a group. The simplest pod definition describes the deployment of a single container. For example, an nginx web server pod might be defined as such: apiVersion : v1 kind : Pod metadata : name : mynginx namespace : default labels : run : nginx spec : containers : - name : mynginx image : nginx:latest ports : - containerPort : 80 Labels \u00b6 In Kubernetes, labels are a system to organize objects into groups. Labels are key-value pairs that are attached to each object. Label selectors can be passed along with a request to the apiserver to retrieve a list of objects which match that label selector. To add a label to a pod, add a labels section under metadata in the pod definition: apiVersion : v1 kind : Pod metadata : labels : run : nginx ... To label a running pod kubectl label pod mynginx type = webserver pod \"mynginx\" labeled To list pods based on labels kubectl get pods -l type = webserver NAME READY STATUS RESTARTS AGE mynginx 1 /1 Running 0 21m Deployments \u00b6 A Deployment provides declarative updates for pods and replicas. You only need to describe the desired state in a Deployment object, and it will change the actual state to the desired state. The Deployment object defines the following details: The elements of a Replication Controller definition The strategy for transitioning between deployments To create a deployment for a nginx webserver, edit the nginx-deploy.yaml file as apiVersion : apps/v1beta1 kind : Deployment metadata : generation : 1 labels : run : nginx name : nginx namespace : default spec : replicas : 3 selector : matchLabels : run : nginx strategy : rollingUpdate : maxSurge : 1 maxUnavailable : 1 type : RollingUpdate template : metadata : labels : run : nginx spec : containers : - image : nginx:latest imagePullPolicy : Always name : nginx ports : - containerPort : 80 protocol : TCP dnsPolicy : ClusterFirst restartPolicy : Always securityContext : {} terminationGracePeriodSeconds : 30 and create the deployment kubectl create -f nginx-deploy.yaml deployment \"nginx\" created The deployment creates the following objects kubectl get all -l run = nginx NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/nginx 3 3 3 3 4m NAME DESIRED CURRENT READY AGE rs/nginx-664452237 3 3 3 4m NAME READY STATUS RESTARTS AGE po/nginx-664452237-h8dh0 1 /1 Running 0 4m po/nginx-664452237-ncsh1 1 /1 Running 0 4m po/nginx-664452237-vts63 1 /1 Running 0 4m services \u00b6 Services Kubernetes pods, as containers, are ephemeral. Replication Controllers create and destroy pods dynamically, e.g. when scaling up or down or when doing rolling updates. While each pod gets its own IP address, even those IP addresses cannot be relied upon to be stable over time. This leads to a problem: if some set of pods provides functionality to other pods inside the Kubernetes cluster, how do those pods find out and keep track of which other? A Kubernetes Service is an abstraction which defines a logical set of pods and a policy by which to access them. The set of pods targeted by a Service is usually determined by a label selector. Kubernetes offers a simple Endpoints API that is updated whenever the set of pods in a service changes. To create a service for our nginx webserver, edit the nginx-service.yaml file apiVersion : v1 kind : Service metadata : name : nginx labels : run : nginx spec : selector : run : nginx ports : - protocol : TCP port : 8000 targetPort : 80 type : ClusterIP Create the service kubectl create -f nginx-service.yaml service \"nginx\" created kubectl get service -l run = nginx NAME CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE nginx 10 .254.60.24 <none> 8000 /TCP 38s Describe the service: kubectl describe service nginx Name: nginx Namespace: default Labels: run = nginx Selector: run = nginx Type: ClusterIP IP: 10 .254.60.24 Port: <unset> 8000 /TCP Endpoints: 172 .30.21.3:80,172.30.4.4:80,172.30.53.4:80 Session Affinity: None No events. The above service is associated to our previous nginx pods. Pay attention to the service selector run=nginx field. It tells Kubernetes that all pods with the label run=nginx are associated to this service, and should have traffic distributed amongst them. In other words, the service provides an abstraction layer, and it is the input point to reach all of the associated pods.","title":"Pod"},{"location":"Lab1/script/script/#pod","text":"In Kubernetes, a group of one or more containers is called a pod. Containers in a pod are deployed together, and are started, stopped, and replicated as a group. The simplest pod definition describes the deployment of a single container. For example, an nginx web server pod might be defined as such: apiVersion : v1 kind : Pod metadata : name : mynginx namespace : default labels : run : nginx spec : containers : - name : mynginx image : nginx:latest ports : - containerPort : 80","title":"Pod"},{"location":"Lab1/script/script/#labels","text":"In Kubernetes, labels are a system to organize objects into groups. Labels are key-value pairs that are attached to each object. Label selectors can be passed along with a request to the apiserver to retrieve a list of objects which match that label selector. To add a label to a pod, add a labels section under metadata in the pod definition: apiVersion : v1 kind : Pod metadata : labels : run : nginx ... To label a running pod kubectl label pod mynginx type = webserver pod \"mynginx\" labeled To list pods based on labels kubectl get pods -l type = webserver NAME READY STATUS RESTARTS AGE mynginx 1 /1 Running 0 21m","title":"Labels"},{"location":"Lab1/script/script/#deployments","text":"A Deployment provides declarative updates for pods and replicas. You only need to describe the desired state in a Deployment object, and it will change the actual state to the desired state. The Deployment object defines the following details: The elements of a Replication Controller definition The strategy for transitioning between deployments To create a deployment for a nginx webserver, edit the nginx-deploy.yaml file as apiVersion : apps/v1beta1 kind : Deployment metadata : generation : 1 labels : run : nginx name : nginx namespace : default spec : replicas : 3 selector : matchLabels : run : nginx strategy : rollingUpdate : maxSurge : 1 maxUnavailable : 1 type : RollingUpdate template : metadata : labels : run : nginx spec : containers : - image : nginx:latest imagePullPolicy : Always name : nginx ports : - containerPort : 80 protocol : TCP dnsPolicy : ClusterFirst restartPolicy : Always securityContext : {} terminationGracePeriodSeconds : 30 and create the deployment kubectl create -f nginx-deploy.yaml deployment \"nginx\" created The deployment creates the following objects kubectl get all -l run = nginx NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/nginx 3 3 3 3 4m NAME DESIRED CURRENT READY AGE rs/nginx-664452237 3 3 3 4m NAME READY STATUS RESTARTS AGE po/nginx-664452237-h8dh0 1 /1 Running 0 4m po/nginx-664452237-ncsh1 1 /1 Running 0 4m po/nginx-664452237-vts63 1 /1 Running 0 4m","title":"Deployments"},{"location":"Lab1/script/script/#services","text":"Services Kubernetes pods, as containers, are ephemeral. Replication Controllers create and destroy pods dynamically, e.g. when scaling up or down or when doing rolling updates. While each pod gets its own IP address, even those IP addresses cannot be relied upon to be stable over time. This leads to a problem: if some set of pods provides functionality to other pods inside the Kubernetes cluster, how do those pods find out and keep track of which other? A Kubernetes Service is an abstraction which defines a logical set of pods and a policy by which to access them. The set of pods targeted by a Service is usually determined by a label selector. Kubernetes offers a simple Endpoints API that is updated whenever the set of pods in a service changes. To create a service for our nginx webserver, edit the nginx-service.yaml file apiVersion : v1 kind : Service metadata : name : nginx labels : run : nginx spec : selector : run : nginx ports : - protocol : TCP port : 8000 targetPort : 80 type : ClusterIP Create the service kubectl create -f nginx-service.yaml service \"nginx\" created kubectl get service -l run = nginx NAME CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE nginx 10 .254.60.24 <none> 8000 /TCP 38s Describe the service: kubectl describe service nginx Name: nginx Namespace: default Labels: run = nginx Selector: run = nginx Type: ClusterIP IP: 10 .254.60.24 Port: <unset> 8000 /TCP Endpoints: 172 .30.21.3:80,172.30.4.4:80,172.30.53.4:80 Session Affinity: None No events. The above service is associated to our previous nginx pods. Pay attention to the service selector run=nginx field. It tells Kubernetes that all pods with the label run=nginx are associated to this service, and should have traffic distributed amongst them. In other words, the service provides an abstraction layer, and it is the input point to reach all of the associated pods.","title":"services"},{"location":"Lab2/","text":"Lab 2: Scale and Update Deployments \u00b6 In this lab, you'll learn how to update the number of instances a deployment has and how to safely roll out an update of your application on Kubernetes. For this lab, you need a running deployment of the guestbook application from the previous lab. If you need to create it, run: kubectl create deployment guestbook --image = ibmcom/guestbook:v1 1. Scale apps with replicas \u00b6 A replica is a copy of a pod that contains a running service. By having multiple replicas of a pod, you can ensure your deployment has the available resources to handle increasing load on your application. kubectl provides a scale subcommand to change the size of an existing deployment. Let's increase our capacity from a single running instance of guestbook up to 10 instances: kubectl scale --replicas = 10 deployment guestbook Kubernetes will now try to make reality match the desired state of 10 replicas by starting 9 new pods with the same configuration as the first. To see your changes being rolled out, you can run: kubectl rollout status deployment guestbook The rollout might occur so quickly that the following messages might not display: $ kubectl rollout status deployment guestbook Waiting for rollout to finish: 1 of 10 updated replicas are available... Waiting for rollout to finish: 2 of 10 updated replicas are available... Waiting for rollout to finish: 3 of 10 updated replicas are available... Waiting for rollout to finish: 4 of 10 updated replicas are available... Waiting for rollout to finish: 5 of 10 updated replicas are available... Waiting for rollout to finish: 6 of 10 updated replicas are available... Waiting for rollout to finish: 7 of 10 updated replicas are available... Waiting for rollout to finish: 8 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Once the rollout has finished, ensure your pods are running by using: kubectl get pods You should see output listing 10 replicas of your deployment: $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-562211614-1tqm7 1 /1 Running 0 1d guestbook-562211614-1zqn4 1 /1 Running 0 2m guestbook-562211614-5htdz 1 /1 Running 0 2m guestbook-562211614-6h04h 1 /1 Running 0 2m guestbook-562211614-ds9hb 1 /1 Running 0 2m guestbook-562211614-nb5qp 1 /1 Running 0 2m guestbook-562211614-vtfp2 1 /1 Running 0 2m guestbook-562211614-vz5qw 1 /1 Running 0 2m guestbook-562211614-zksw3 1 /1 Running 0 2m guestbook-562211614-zsp0j 1 /1 Running 0 2m Tip: Another way to improve availability is to add clusters and regions to your deployment, as shown in the following diagram: 2. Update and roll back apps \u00b6 Kubernetes allows you to do rolling upgrade of your application to a new container image. This allows you to easily update the running image and also allows you to easily undo a rollout if a problem is discovered during or after deployment. In the previous lab, we used an image with a v1 tag. For our upgrade we'll use the image with the v2 tag. To update and roll back: Using kubectl , you can now update your deployment to use the v2 image. kubectl allows you to change details about existing resources with the set subcommand. We can use it to change the image being used. kubectl set image deployment/guestbook guestbook = ibmcom/guestbook:v2 Note that a pod could have multiple containers, each with its own name. Each image can be changed individually or all at once by referring to the name. In the case of our guestbook Deployment, the container name is also guestbook . Multiple containers can be updated at the same time. ( More information .) To check the status of the rollout, run: kubectl rollout status deployment/guestbook The rollout might occur so quickly that the following messages might not display: $ kubectl rollout status deployment/guestbook Waiting for rollout to finish: 2 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Test the application as before, by accessing <public-IP>:<nodeport> in the browser to confirm your new code is active. Remember, to get the \"nodeport\" and \"public-ip\" use the following commands. Replace $CLUSTER_NAME with the name of your cluster if the environment variable is not set.: kubectl describe service guestbook and kubectl get nodes -o wide To verify that you're running \"v2\" of guestbook, look at the title of the page, it should now be Guestbook - v2 . If you are using a browser, make sure you force refresh (invalidating your cache). If you want to undo your latest rollout, use: kubectl rollout undo deployment guestbook You can then use this command to see the status: kubectl rollout status deployment/guestbook When doing a rollout, you see references to old replicas and new replicas. The old replicas are the original 10 pods deployed when we scaled the application. The new replicas come from the newly created pods with the different image. All of these pods are owned by the Deployment. The deployment manages these two sets of pods with a resource called a ReplicaSet. We can see the guestbook ReplicaSets with: $ kubectl get replicasets -l app = guestbook NAME DESIRED CURRENT READY AGE guestbook-5f5548d4f 10 10 10 21m guestbook-768cc55c78 0 0 0 3h Before we continue, let's delete the application so we can learn about a different way to achieve the same results: To remove the deployment, use kubectl delete deployment guestbook To remove the service, use: kubectl delete service guestbook Congratulations! You deployed the second version of the app. Lab 2 is now complete. Continue to the next lab of this course .","title":"Lab 2. Scale and update deployments"},{"location":"Lab2/#lab-2-scale-and-update-deployments","text":"In this lab, you'll learn how to update the number of instances a deployment has and how to safely roll out an update of your application on Kubernetes. For this lab, you need a running deployment of the guestbook application from the previous lab. If you need to create it, run: kubectl create deployment guestbook --image = ibmcom/guestbook:v1","title":"Lab 2: Scale and Update Deployments"},{"location":"Lab2/#1-scale-apps-with-replicas","text":"A replica is a copy of a pod that contains a running service. By having multiple replicas of a pod, you can ensure your deployment has the available resources to handle increasing load on your application. kubectl provides a scale subcommand to change the size of an existing deployment. Let's increase our capacity from a single running instance of guestbook up to 10 instances: kubectl scale --replicas = 10 deployment guestbook Kubernetes will now try to make reality match the desired state of 10 replicas by starting 9 new pods with the same configuration as the first. To see your changes being rolled out, you can run: kubectl rollout status deployment guestbook The rollout might occur so quickly that the following messages might not display: $ kubectl rollout status deployment guestbook Waiting for rollout to finish: 1 of 10 updated replicas are available... Waiting for rollout to finish: 2 of 10 updated replicas are available... Waiting for rollout to finish: 3 of 10 updated replicas are available... Waiting for rollout to finish: 4 of 10 updated replicas are available... Waiting for rollout to finish: 5 of 10 updated replicas are available... Waiting for rollout to finish: 6 of 10 updated replicas are available... Waiting for rollout to finish: 7 of 10 updated replicas are available... Waiting for rollout to finish: 8 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Once the rollout has finished, ensure your pods are running by using: kubectl get pods You should see output listing 10 replicas of your deployment: $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-562211614-1tqm7 1 /1 Running 0 1d guestbook-562211614-1zqn4 1 /1 Running 0 2m guestbook-562211614-5htdz 1 /1 Running 0 2m guestbook-562211614-6h04h 1 /1 Running 0 2m guestbook-562211614-ds9hb 1 /1 Running 0 2m guestbook-562211614-nb5qp 1 /1 Running 0 2m guestbook-562211614-vtfp2 1 /1 Running 0 2m guestbook-562211614-vz5qw 1 /1 Running 0 2m guestbook-562211614-zksw3 1 /1 Running 0 2m guestbook-562211614-zsp0j 1 /1 Running 0 2m Tip: Another way to improve availability is to add clusters and regions to your deployment, as shown in the following diagram:","title":"1. Scale apps with replicas"},{"location":"Lab2/#2-update-and-roll-back-apps","text":"Kubernetes allows you to do rolling upgrade of your application to a new container image. This allows you to easily update the running image and also allows you to easily undo a rollout if a problem is discovered during or after deployment. In the previous lab, we used an image with a v1 tag. For our upgrade we'll use the image with the v2 tag. To update and roll back: Using kubectl , you can now update your deployment to use the v2 image. kubectl allows you to change details about existing resources with the set subcommand. We can use it to change the image being used. kubectl set image deployment/guestbook guestbook = ibmcom/guestbook:v2 Note that a pod could have multiple containers, each with its own name. Each image can be changed individually or all at once by referring to the name. In the case of our guestbook Deployment, the container name is also guestbook . Multiple containers can be updated at the same time. ( More information .) To check the status of the rollout, run: kubectl rollout status deployment/guestbook The rollout might occur so quickly that the following messages might not display: $ kubectl rollout status deployment/guestbook Waiting for rollout to finish: 2 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Test the application as before, by accessing <public-IP>:<nodeport> in the browser to confirm your new code is active. Remember, to get the \"nodeport\" and \"public-ip\" use the following commands. Replace $CLUSTER_NAME with the name of your cluster if the environment variable is not set.: kubectl describe service guestbook and kubectl get nodes -o wide To verify that you're running \"v2\" of guestbook, look at the title of the page, it should now be Guestbook - v2 . If you are using a browser, make sure you force refresh (invalidating your cache). If you want to undo your latest rollout, use: kubectl rollout undo deployment guestbook You can then use this command to see the status: kubectl rollout status deployment/guestbook When doing a rollout, you see references to old replicas and new replicas. The old replicas are the original 10 pods deployed when we scaled the application. The new replicas come from the newly created pods with the different image. All of these pods are owned by the Deployment. The deployment manages these two sets of pods with a resource called a ReplicaSet. We can see the guestbook ReplicaSets with: $ kubectl get replicasets -l app = guestbook NAME DESIRED CURRENT READY AGE guestbook-5f5548d4f 10 10 10 21m guestbook-768cc55c78 0 0 0 3h Before we continue, let's delete the application so we can learn about a different way to achieve the same results: To remove the deployment, use kubectl delete deployment guestbook To remove the service, use: kubectl delete service guestbook Congratulations! You deployed the second version of the app. Lab 2 is now complete. Continue to the next lab of this course .","title":"2. Update and roll back apps"},{"location":"Lab3/","text":"Lab 3: Scale and update apps natively, building multi-tier applications \u00b6 In this lab you'll learn how to deploy the same guestbook application we deployed in the previous labs, however, instead of using the kubectl command line helper functions we'll be deploying the application using configuration files. The configuration file mechanism allows you to have more fine-grained control over all of resources being created within the Kubernetes cluster. Before we work with the application we need to clone a github repo: git clone https://github.com/IBM/guestbook.git This repo contains multiple versions of the guestbook application as well as the configuration files we'll use to deploy the pieces of the application. Change directory by running the command cd guestbook/v1 You will find all the configurations files for this exercise in this directory. 1. Scale apps natively \u00b6 Kubernetes can deploy an individual pod to run an application but when you need to scale it to handle a large number of requests a Deployment is the resource you want to use. A Deployment manages a collection of similar pods. When you ask for a specific number of replicas the Kubernetes Deployment Controller will attempt to maintain that number of replicas at all times. Every Kubernetes object we create should provide two nested object fields that govern the object\u2019s configuration: the object spec and the object status . Object spec defines the desired state, and object status contains Kubernetes system provided information about the actual state of the resource. As described before, Kubernetes will attempt to reconcile your desired state with the actual state of the system. For Object that we create we need to provide the apiVersion you are using to create the object, kind of the object we are creating and the metadata about the object such as a name , set of labels and optionally namespace that this object should belong. Consider the following deployment configuration for guestbook application guestbook-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 labels : app : guestbook version : \"1.0\" spec : replicas : 3 selector : matchLabels : app : guestbook template : metadata : labels : app : guestbook version : \"1.0\" spec : containers : - name : guestbook image : ibmcom/guestbook:v1 ports : - name : http-server containerPort : 3000 The above configuration file create a deployment object named 'guestbook' with a pod containing a single container running the image ibmcom/guestbook:v1 . Also the configuration specifies replicas set to 3 and Kubernetes tries to make sure that at least three active pods are running at all times. Create guestbook deployment To create a Deployment using this configuration file we use the following command: kubectl create -f guestbook-deployment.yaml List the pod with label app=guestbook We can then list the pods it created by listing all pods that have a label of \"app\" with a value of \"guestbook\". This matches the labels defined above in the yaml file in the spec.template.metadata.labels section. kubectl get pods -l app = guestbook When you change the number of replicas in the configuration, Kubernetes will try to add, or remove, pods from the system to match your request. To can make these modifications by using the following command: kubectl edit deployment guestbook-v1 This will retrieve the latest configuration for the Deployment from the Kubernetes server and then load it into an editor for you. You'll notice that there are a lot more fields in this version than the original yaml file we used. This is because it contains all of the properties about the Deployment that Kubernetes knows about, not just the ones we chose to specify when we create it. Also notice that it now contains the status section mentioned previously. To exit the vi editor, type :q! , of if you made changes that you want to see reflected, save them using :wq . You can also edit the deployment file we used to create the Deployment to make changes. You should use the following command to make the change effective when you edit the deployment locally. kubectl apply -f guestbook-deployment.yaml This will ask Kubernetes to \"diff\" our yaml file with the current state of the Deployment and apply just those changes. We can now define a Service object to expose the deployment to external clients. guestbook-service.yaml apiVersion : v1 kind : Service metadata : name : guestbook labels : app : guestbook spec : ports : - port : 3000 targetPort : http-server selector : app : guestbook type : LoadBalancer The above configuration creates a Service resource named guestbook. A Service can be used to create a network path for incoming traffic to your running application. In this case, we are setting up a route from port 3000 on the cluster to the \"http-server\" port on our app, which is port 3000 per the Deployment container spec. Let us now create the guestbook service using the same type of command we used when we created the Deployment: kubectl create -f guestbook-service.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> Remember, to get the nodeport and public-ip use the following commands, replacing $CLUSTER_NAME with the name of your cluster if the environment variable is not already set. kubectl describe service guestbook and kubectl get nodes -o wide 2. Connect to a back-end service \u00b6 If you look at the guestbook source code, under the guestbook/v1/guestbook directory, you'll notice that it is written to support a variety of data stores. By default it will keep the log of guestbook entries in memory. That's ok for testing purposes, but as you get into a more \"real\" environment where you scale your application that model will not work because based on which instance of the application the user is routed to they'll see very different results. To solve this we need to have all instances of our app share the same data store - in this case we're going to use a redis database that we deploy to our cluster. This instance of redis will be defined in a similar manner to the guestbook. redis-master-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-master labels : app : redis role : master spec : replicas : 1 selector : matchLabels : app : redis role : master template : metadata : labels : app : redis role : master spec : containers : - name : redis-master image : redis:3.2.9 ports : - name : redis-server containerPort : 6379 This yaml creates a redis database in a Deployment named 'redis-master'. It will create a single instance, with replicas set to 1, and the guestbook app instances will connect to it to persist data, as well as read the persisted data back. The image running in the container is 'redis:3.2.9' and exposes the standard redis port 6379. Create a redis Deployment, like we did for guestbook: kubectl create -f redis-master-deployment.yaml Check to see that redis server pod is running: $ kubectl get pods -lapp = redis,role = master NAME READY STATUS RESTARTS AGE redis-master-q9zg7 1 /1 Running 0 2d Let us test the redis standalone. Replace the pod name redis-master-q9zg7 with the name of your pod. kubectl exec -it redis-master-q9zg7 redis-cli The kubectl exec command will start a secondary process in the specified container. In this case we're asking for the \"redis-cli\" command to be executed in the container named \"redis-master-q9zg7\". When this process ends the \"kubectl exec\" command will also exit but the other processes in the container will not be impacted. Once in the container we can use the \"redis-cli\" command to make sure the redis database is running properly, or to configure it if needed. redis-cli> ping PONG redis-cli> exit Now we need to expose the redis-master Deployment as a Service so that the guestbook application can connect to it through DNS lookup. redis-master-service.yaml apiVersion : v1 kind : Service metadata : name : redis-master labels : app : redis role : master spec : ports : - port : 6379 targetPort : redis-server selector : app : redis role : master This creates a Service object named 'redis-master' and configures it to target port 6379 on the pods selected by the selectors \"app=redis\" and \"role=master\". Create the service to access redis master: kubectl create -f redis-master-service.yaml Restart guestbook so that it will find the redis service to use database: kubectl delete deploy guestbook-v1 kubectl create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> , or by refreshing the page if you already have the app open in another window. You can see now that if you open up multiple browsers and refresh the page to access the different copies of guestbook that they all have a consistent state. All instances write to the same backing persistent storage, and all instances read from that storage to display the guestbook entries that have been stored. We have our simple 3-tier application running but we need to scale the application if traffic increases. Our main bottleneck is that we only have one database server to process each request coming though guestbook. One simple solution is to separate the reads and write such that they go to different databases that are replicated properly to achieve data consistency. Create a deployment named 'redis-slave' that can talk to redis database to manage data reads. In order to scale the database we use the pattern where we can scale the reads using redis slave deployment which can run several instances to read. Redis slave deployments is configured to run two replicas. redis-slave-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-slave labels : app : redis role : slave spec : replicas : 2 selector : matchLabels : app : redis role : slave template : metadata : labels : app : redis role : slave spec : containers : - name : redis-slave image : ibmcom/guestbook-redis-slave:v2 ports : - name : redis-server containerPort : 6379 Create the pod running redis slave deployment. kubectl create -f redis-slave-deployment.yaml Check if all the slave replicas are running $ kubectl get pods -lapp = redis,role = slave NAME READY STATUS RESTARTS AGE redis-slave-kd7vx 1 /1 Running 0 2d redis-slave-wwcxw 1 /1 Running 0 2d And then go into one of those pods and look at the database to see that everything looks right. Replace the pod name redis-slave-kd7vx with your own pod name. If you get the back (empty list or set) when you print the keys, go to the guestbook application and add an entry! $ kubectl exec -it redis-slave-kd7vx redis-cli 127 .0.0.1:6379> keys * 1 ) \"guestbook\" 127 .0.0.1:6379> lrange guestbook 0 10 1 ) \"hello world\" 2 ) \"welcome to the Kube workshop\" 127 .0.0.1:6379> exit Deploy redis slave service so we can access it by DNS name. Once redeployed, the application will send \"read\" operations to the redis-slave pods while \"write\" operations will go to the redis-master pods. redis-slave-service.yaml apiVersion : v1 kind : Service metadata : name : redis-slave labels : app : redis role : slave spec : ports : - port : 6379 targetPort : redis-server selector : app : redis role : slave Create the service to access redis slaves. kubectl create -f redis-slave-service.yaml Restart guestbook so that it will find the slave service to read from. kubectl delete deploy guestbook-v1 kubectl create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> , or by refreshing the page if you have the app open in another window. That's the end of the lab. Now let's clean-up our environment: kubectl delete -f guestbook-deployment.yaml kubectl delete -f guestbook-service.yaml kubectl delete -f redis-slave-service.yaml kubectl delete -f redis-slave-deployment.yaml kubectl delete -f redis-master-service.yaml kubectl delete -f redis-master-deployment.yaml","title":"Lab 3. Build multi-tier applications"},{"location":"Lab3/#lab-3-scale-and-update-apps-natively-building-multi-tier-applications","text":"In this lab you'll learn how to deploy the same guestbook application we deployed in the previous labs, however, instead of using the kubectl command line helper functions we'll be deploying the application using configuration files. The configuration file mechanism allows you to have more fine-grained control over all of resources being created within the Kubernetes cluster. Before we work with the application we need to clone a github repo: git clone https://github.com/IBM/guestbook.git This repo contains multiple versions of the guestbook application as well as the configuration files we'll use to deploy the pieces of the application. Change directory by running the command cd guestbook/v1 You will find all the configurations files for this exercise in this directory.","title":"Lab 3: Scale and update apps natively, building multi-tier applications"},{"location":"Lab3/#1-scale-apps-natively","text":"Kubernetes can deploy an individual pod to run an application but when you need to scale it to handle a large number of requests a Deployment is the resource you want to use. A Deployment manages a collection of similar pods. When you ask for a specific number of replicas the Kubernetes Deployment Controller will attempt to maintain that number of replicas at all times. Every Kubernetes object we create should provide two nested object fields that govern the object\u2019s configuration: the object spec and the object status . Object spec defines the desired state, and object status contains Kubernetes system provided information about the actual state of the resource. As described before, Kubernetes will attempt to reconcile your desired state with the actual state of the system. For Object that we create we need to provide the apiVersion you are using to create the object, kind of the object we are creating and the metadata about the object such as a name , set of labels and optionally namespace that this object should belong. Consider the following deployment configuration for guestbook application guestbook-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 labels : app : guestbook version : \"1.0\" spec : replicas : 3 selector : matchLabels : app : guestbook template : metadata : labels : app : guestbook version : \"1.0\" spec : containers : - name : guestbook image : ibmcom/guestbook:v1 ports : - name : http-server containerPort : 3000 The above configuration file create a deployment object named 'guestbook' with a pod containing a single container running the image ibmcom/guestbook:v1 . Also the configuration specifies replicas set to 3 and Kubernetes tries to make sure that at least three active pods are running at all times. Create guestbook deployment To create a Deployment using this configuration file we use the following command: kubectl create -f guestbook-deployment.yaml List the pod with label app=guestbook We can then list the pods it created by listing all pods that have a label of \"app\" with a value of \"guestbook\". This matches the labels defined above in the yaml file in the spec.template.metadata.labels section. kubectl get pods -l app = guestbook When you change the number of replicas in the configuration, Kubernetes will try to add, or remove, pods from the system to match your request. To can make these modifications by using the following command: kubectl edit deployment guestbook-v1 This will retrieve the latest configuration for the Deployment from the Kubernetes server and then load it into an editor for you. You'll notice that there are a lot more fields in this version than the original yaml file we used. This is because it contains all of the properties about the Deployment that Kubernetes knows about, not just the ones we chose to specify when we create it. Also notice that it now contains the status section mentioned previously. To exit the vi editor, type :q! , of if you made changes that you want to see reflected, save them using :wq . You can also edit the deployment file we used to create the Deployment to make changes. You should use the following command to make the change effective when you edit the deployment locally. kubectl apply -f guestbook-deployment.yaml This will ask Kubernetes to \"diff\" our yaml file with the current state of the Deployment and apply just those changes. We can now define a Service object to expose the deployment to external clients. guestbook-service.yaml apiVersion : v1 kind : Service metadata : name : guestbook labels : app : guestbook spec : ports : - port : 3000 targetPort : http-server selector : app : guestbook type : LoadBalancer The above configuration creates a Service resource named guestbook. A Service can be used to create a network path for incoming traffic to your running application. In this case, we are setting up a route from port 3000 on the cluster to the \"http-server\" port on our app, which is port 3000 per the Deployment container spec. Let us now create the guestbook service using the same type of command we used when we created the Deployment: kubectl create -f guestbook-service.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> Remember, to get the nodeport and public-ip use the following commands, replacing $CLUSTER_NAME with the name of your cluster if the environment variable is not already set. kubectl describe service guestbook and kubectl get nodes -o wide","title":"1. Scale apps natively"},{"location":"Lab3/#2-connect-to-a-back-end-service","text":"If you look at the guestbook source code, under the guestbook/v1/guestbook directory, you'll notice that it is written to support a variety of data stores. By default it will keep the log of guestbook entries in memory. That's ok for testing purposes, but as you get into a more \"real\" environment where you scale your application that model will not work because based on which instance of the application the user is routed to they'll see very different results. To solve this we need to have all instances of our app share the same data store - in this case we're going to use a redis database that we deploy to our cluster. This instance of redis will be defined in a similar manner to the guestbook. redis-master-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-master labels : app : redis role : master spec : replicas : 1 selector : matchLabels : app : redis role : master template : metadata : labels : app : redis role : master spec : containers : - name : redis-master image : redis:3.2.9 ports : - name : redis-server containerPort : 6379 This yaml creates a redis database in a Deployment named 'redis-master'. It will create a single instance, with replicas set to 1, and the guestbook app instances will connect to it to persist data, as well as read the persisted data back. The image running in the container is 'redis:3.2.9' and exposes the standard redis port 6379. Create a redis Deployment, like we did for guestbook: kubectl create -f redis-master-deployment.yaml Check to see that redis server pod is running: $ kubectl get pods -lapp = redis,role = master NAME READY STATUS RESTARTS AGE redis-master-q9zg7 1 /1 Running 0 2d Let us test the redis standalone. Replace the pod name redis-master-q9zg7 with the name of your pod. kubectl exec -it redis-master-q9zg7 redis-cli The kubectl exec command will start a secondary process in the specified container. In this case we're asking for the \"redis-cli\" command to be executed in the container named \"redis-master-q9zg7\". When this process ends the \"kubectl exec\" command will also exit but the other processes in the container will not be impacted. Once in the container we can use the \"redis-cli\" command to make sure the redis database is running properly, or to configure it if needed. redis-cli> ping PONG redis-cli> exit Now we need to expose the redis-master Deployment as a Service so that the guestbook application can connect to it through DNS lookup. redis-master-service.yaml apiVersion : v1 kind : Service metadata : name : redis-master labels : app : redis role : master spec : ports : - port : 6379 targetPort : redis-server selector : app : redis role : master This creates a Service object named 'redis-master' and configures it to target port 6379 on the pods selected by the selectors \"app=redis\" and \"role=master\". Create the service to access redis master: kubectl create -f redis-master-service.yaml Restart guestbook so that it will find the redis service to use database: kubectl delete deploy guestbook-v1 kubectl create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> , or by refreshing the page if you already have the app open in another window. You can see now that if you open up multiple browsers and refresh the page to access the different copies of guestbook that they all have a consistent state. All instances write to the same backing persistent storage, and all instances read from that storage to display the guestbook entries that have been stored. We have our simple 3-tier application running but we need to scale the application if traffic increases. Our main bottleneck is that we only have one database server to process each request coming though guestbook. One simple solution is to separate the reads and write such that they go to different databases that are replicated properly to achieve data consistency. Create a deployment named 'redis-slave' that can talk to redis database to manage data reads. In order to scale the database we use the pattern where we can scale the reads using redis slave deployment which can run several instances to read. Redis slave deployments is configured to run two replicas. redis-slave-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-slave labels : app : redis role : slave spec : replicas : 2 selector : matchLabels : app : redis role : slave template : metadata : labels : app : redis role : slave spec : containers : - name : redis-slave image : ibmcom/guestbook-redis-slave:v2 ports : - name : redis-server containerPort : 6379 Create the pod running redis slave deployment. kubectl create -f redis-slave-deployment.yaml Check if all the slave replicas are running $ kubectl get pods -lapp = redis,role = slave NAME READY STATUS RESTARTS AGE redis-slave-kd7vx 1 /1 Running 0 2d redis-slave-wwcxw 1 /1 Running 0 2d And then go into one of those pods and look at the database to see that everything looks right. Replace the pod name redis-slave-kd7vx with your own pod name. If you get the back (empty list or set) when you print the keys, go to the guestbook application and add an entry! $ kubectl exec -it redis-slave-kd7vx redis-cli 127 .0.0.1:6379> keys * 1 ) \"guestbook\" 127 .0.0.1:6379> lrange guestbook 0 10 1 ) \"hello world\" 2 ) \"welcome to the Kube workshop\" 127 .0.0.1:6379> exit Deploy redis slave service so we can access it by DNS name. Once redeployed, the application will send \"read\" operations to the redis-slave pods while \"write\" operations will go to the redis-master pods. redis-slave-service.yaml apiVersion : v1 kind : Service metadata : name : redis-slave labels : app : redis role : slave spec : ports : - port : 6379 targetPort : redis-server selector : app : redis role : slave Create the service to access redis slaves. kubectl create -f redis-slave-service.yaml Restart guestbook so that it will find the slave service to read from. kubectl delete deploy guestbook-v1 kubectl create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> , or by refreshing the page if you have the app open in another window. That's the end of the lab. Now let's clean-up our environment: kubectl delete -f guestbook-deployment.yaml kubectl delete -f guestbook-service.yaml kubectl delete -f redis-slave-service.yaml kubectl delete -f redis-slave-deployment.yaml kubectl delete -f redis-master-service.yaml kubectl delete -f redis-master-deployment.yaml","title":"2. Connect to a back-end service"},{"location":"Lab4/","text":"UNDER CONSTRUCTION \u00b6 1. Check the health of apps \u00b6 Kubernetes uses availability checks (liveness probes) to know when to restart a container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a container in such a state can help to make the application more available despite bugs. Also, Kubernetes uses readiness checks to know when a container is ready to start accepting traffic. A pod is considered ready when all of its containers are ready. One use of this check is to control which pods are used as backends for services. When a pod is not ready, it is removed from load balancers. In this example, we have defined a HTTP liveness probe to check health of the container every five seconds. For the first 10-15 seconds the /healthz returns a 200 response and will fail afterward. Kubernetes will automatically restart the service. Open the healthcheck.yml file with a text editor. This configuration script combines a few steps from the previous lesson to create a deployment and a service at the same time. App developers can use these scripts when updates are made or to troubleshoot issues by re-creating the pods: Update the details for the image in your private registry namespace: image : \"ibmcom/guestbook:v2\" Note the HTTP liveness probe that checks the health of the container every five seconds. livenessProbe : httpGet : path : /healthz port : 3000 initialDelaySeconds : 5 periodSeconds : 5 In the Service section, note the NodePort . Rather than generating a random NodePort like you did in the previous lesson, you can specify a port in the 30000 - 32767 range. This example uses 30072. Run the configuration script in the cluster. When the deployment and the service are created, the app is available for anyone to see: kubectl apply -f healthcheck.yml Now that all the deployment work is done, check how everything turned out. You might notice that because more instances are running, things might run a bit slower. Open a browser and check out the app. To form the URL, combine the IP with the NodePort that was specified in the configuration script. To get the public IP address for the worker node: ibmcloud cs workers <cluster-name> In a browser, you'll see a success message. If you do not see this text, don't worry. This app is designed to go up and down. For the first 10 - 15 seconds, a 200 message is returned, so you know that the app is running successfully. After those 15 seconds, a timeout message is displayed, as is designed in the app. Launch your Kubernetes dashboard: Get your credentials for Kubernetes. kubectl config view -o jsonpath = '{.users[0].user.auth-provider.config.id-token}' Copy the id-token value that is shown in the output. Set the proxy with the default port number. kubectl proxy Output: Starting to serve on 127 .0.0.1:8001 Sign in to the dashboard. Open the following URL in a web browser. http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ In the sign-on page, select the Token authentication method. Then, paste the id-token value that you previously copied into the Token field and click SIGN IN . In the Workloads tab, you can see the resources that you created. From this tab, you can continually refresh and see that the health check is working. In the Pods section, you can see how many times the pods are restarted when the containers in them are re-created. You might happen to catch errors in the dashboard, indicating that the health check caught a problem. Give it a few minutes and refresh again. You see the number of restarts changes for each pod. Ready to delete what you created before you continue? This time, you can use the same configuration script to delete both of the resources you created. kubectl delete -f healthcheck.yml When you are done exploring the Kubernetes dashboard, in your CLI, enter CTRL+C to exit the proxy command.","title":"***UNDER CONSTRUCTION***"},{"location":"Lab4/#under-construction","text":"","title":"UNDER CONSTRUCTION"},{"location":"Lab4/#1-check-the-health-of-apps","text":"Kubernetes uses availability checks (liveness probes) to know when to restart a container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a container in such a state can help to make the application more available despite bugs. Also, Kubernetes uses readiness checks to know when a container is ready to start accepting traffic. A pod is considered ready when all of its containers are ready. One use of this check is to control which pods are used as backends for services. When a pod is not ready, it is removed from load balancers. In this example, we have defined a HTTP liveness probe to check health of the container every five seconds. For the first 10-15 seconds the /healthz returns a 200 response and will fail afterward. Kubernetes will automatically restart the service. Open the healthcheck.yml file with a text editor. This configuration script combines a few steps from the previous lesson to create a deployment and a service at the same time. App developers can use these scripts when updates are made or to troubleshoot issues by re-creating the pods: Update the details for the image in your private registry namespace: image : \"ibmcom/guestbook:v2\" Note the HTTP liveness probe that checks the health of the container every five seconds. livenessProbe : httpGet : path : /healthz port : 3000 initialDelaySeconds : 5 periodSeconds : 5 In the Service section, note the NodePort . Rather than generating a random NodePort like you did in the previous lesson, you can specify a port in the 30000 - 32767 range. This example uses 30072. Run the configuration script in the cluster. When the deployment and the service are created, the app is available for anyone to see: kubectl apply -f healthcheck.yml Now that all the deployment work is done, check how everything turned out. You might notice that because more instances are running, things might run a bit slower. Open a browser and check out the app. To form the URL, combine the IP with the NodePort that was specified in the configuration script. To get the public IP address for the worker node: ibmcloud cs workers <cluster-name> In a browser, you'll see a success message. If you do not see this text, don't worry. This app is designed to go up and down. For the first 10 - 15 seconds, a 200 message is returned, so you know that the app is running successfully. After those 15 seconds, a timeout message is displayed, as is designed in the app. Launch your Kubernetes dashboard: Get your credentials for Kubernetes. kubectl config view -o jsonpath = '{.users[0].user.auth-provider.config.id-token}' Copy the id-token value that is shown in the output. Set the proxy with the default port number. kubectl proxy Output: Starting to serve on 127 .0.0.1:8001 Sign in to the dashboard. Open the following URL in a web browser. http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ In the sign-on page, select the Token authentication method. Then, paste the id-token value that you previously copied into the Token field and click SIGN IN . In the Workloads tab, you can see the resources that you created. From this tab, you can continually refresh and see that the health check is working. In the Pods section, you can see how many times the pods are restarted when the containers in them are re-created. You might happen to catch errors in the dashboard, indicating that the health check caught a problem. Give it a few minutes and refresh again. You see the number of restarts changes for each pod. Ready to delete what you created before you continue? This time, you can use the same configuration script to delete both of the resources you created. kubectl delete -f healthcheck.yml When you are done exploring the Kubernetes dashboard, in your CLI, enter CTRL+C to exit the proxy command.","title":"1. Check the health of apps"},{"location":"LabD/","text":"Optional Debugging Lab - Tips and Tricks for Debugging Applications in Kubernetes \u00b6 Advanced debugging techniques to reach your pods. Pod Logs \u00b6 You can look at the logs of any of the pods running under your deployments as follows kubectl logs <podname> Remember that if you have multiple containers running in your pod, you have to specify the specific container you want to see logs from. kubectl logs <pod-name> <container-name> This subcommand operates like tail . Including the -f flag will continue to stream the logs live once the current time is reached. kubectl edit and vi \u00b6 By default, on many Linux and macOS systems, you will be dropped into the editor vi . export EDITOR = nano On Windows, a copy of notepad.exe will be opened with the contents of the file. busybox pod \u00b6 For debugging live, this command frequently helps me: kubectl create deployment bb --image busybox --restart = Never -it --rm In the busybox image is a basic shell that contains useful utilities. Utils I often use are nslookup and wget . nslookup is useful for testing DNS resolution in a pod. wget is useful for trying to do network requests. Service Endpoints \u00b6 Endpoint resource can be used to see all the service endpoints. kubectl get endpoints <service> ImagePullPolicy \u00b6 By default Kubernetes will only pull the image on first use. This can be confusing during development when you expect changes to show up. You should be aware of the three ImagePullPolicy s: IfNotPresent - the default, only request the image if not present. Always - always request the image. Never More details on image management may be found here .","title":"Optional Debugging Lab - Tips and Tricks for Debugging Applications in Kubernetes"},{"location":"LabD/#optional-debugging-lab-tips-and-tricks-for-debugging-applications-in-kubernetes","text":"Advanced debugging techniques to reach your pods.","title":"Optional Debugging Lab - Tips and Tricks for Debugging Applications in Kubernetes"},{"location":"LabD/#pod-logs","text":"You can look at the logs of any of the pods running under your deployments as follows kubectl logs <podname> Remember that if you have multiple containers running in your pod, you have to specify the specific container you want to see logs from. kubectl logs <pod-name> <container-name> This subcommand operates like tail . Including the -f flag will continue to stream the logs live once the current time is reached.","title":"Pod Logs"},{"location":"LabD/#kubectl-edit-and-vi","text":"By default, on many Linux and macOS systems, you will be dropped into the editor vi . export EDITOR = nano On Windows, a copy of notepad.exe will be opened with the contents of the file.","title":"kubectl edit and vi"},{"location":"LabD/#busybox-pod","text":"For debugging live, this command frequently helps me: kubectl create deployment bb --image busybox --restart = Never -it --rm In the busybox image is a basic shell that contains useful utilities. Utils I often use are nslookup and wget . nslookup is useful for testing DNS resolution in a pod. wget is useful for trying to do network requests.","title":"busybox pod"},{"location":"LabD/#service-endpoints","text":"Endpoint resource can be used to see all the service endpoints. kubectl get endpoints <service>","title":"Service Endpoints"},{"location":"LabD/#imagepullpolicy","text":"By default Kubernetes will only pull the image on first use. This can be confusing during development when you expect changes to show up. You should be aware of the three ImagePullPolicy s: IfNotPresent - the default, only request the image if not present. Always - always request the image. Never More details on image management may be found here .","title":"ImagePullPolicy"}]}